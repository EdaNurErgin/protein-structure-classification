# -*- coding: utf-8 -*-
"""biyoenformatik.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vQquFR2e9JzPXbxve8NsKVwl6KClFdPS
"""

!pip install biopython scikit-learn tensorflow openpyxl

import numpy as np
import pandas as pd

from Bio import SeqIO

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    matthews_corrcoef, cohen_kappa_score,
    roc_auc_score, confusion_matrix
)
from sklearn.preprocessing import label_binarize

import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
from tensorflow.keras.utils import to_categorical

from google.colab import files

uploaded = files.upload()

# Heliks eğilimi (Pa)
Pa = {
    "A": 1.45, "R": 1.00, "N": 0.67, "D": 1.01, "C": 0.77,
    "Q": 1.11, "E": 1.51, "G": 0.57, "H": 1.00, "I": 1.08,
    "L": 1.34, "K": 1.07, "M": 1.20, "F": 1.12, "P": 0.57,
    "S": 0.77, "T": 0.83, "W": 1.14, "Y": 0.61, "V": 1.06
}

# Beta sheet eğilimi (Pb)
Pb = {
    "A": 0.97, "R": 0.90, "N": 0.89, "D": 0.54, "C": 1.30,
    "Q": 1.10, "E": 0.37, "G": 0.75, "H": 0.87, "I": 1.60,
    "L": 1.22, "K": 0.74, "M": 1.67, "F": 1.28, "P": 0.55,
    "S": 0.75, "T": 1.19, "W": 1.19, "Y": 1.29, "V": 1.70
}

# Turn eğilimi (Pt) – örnek tablo
Pt = {
    "A": 0.66, "R": 0.95, "N": 1.56, "D": 1.46, "C": 1.19,
    "Q": 0.98, "E": 0.74, "G": 1.56, "H": 0.95, "I": 0.47,
    "L": 0.59, "K": 1.01, "M": 0.60, "F": 0.60, "P": 1.52,
    "S": 1.43, "T": 0.96, "W": 0.96, "Y": 1.14, "V": 0.50
}

fasta_name = "proteinStructure.fasta"

sequences = []
labels = []
ids = []

for record in SeqIO.parse(fasta_name, "fasta"):
    seq = str(record.seq)
    sequences.append(seq)
    ids.append(record.id)

    # skorları topla
    h_score = sum(Pa.get(a, 0) for a in seq)
    b_score = sum(Pb.get(a, 0) for a in seq)
    t_score = sum(Pt.get(a, 0) for a in seq)

    scores = {
        "Helix": h_score,
        "Sheet": b_score,
        "Turn": t_score
    }

    label = max(scores, key=scores.get)
    labels.append(label)

df_cf = pd.DataFrame({
    "ProteinID": ids,
    "Sequence": sequences,
    "Class": labels
})

df_cf.head()

# Chou-Fasman sonuçlarını Excel dosyasına kaydet
cf_output_path = "chou_fasman_results.xlsx"
df_cf.to_excel(cf_output_path, index=False)
print(f"Chou-Fasman sonuçları '{cf_output_path}' dosyasına kaydedildi.")

EIIP = {
    "A": 0.0373, "C": 0.0829, "D": 0.1260, "E": 0.0058,
    "F": 0.0946, "G": 0.0050, "H": 0.0242, "I": 0.0946,
    "K": 0.0371, "L": 0.0959, "M": 0.0823, "N": 0.0036,
    "P": 0.0198, "Q": 0.0761, "R": 0.0959, "S": 0.0829,
    "T": 0.0941, "V": 0.0057, "W": 0.0548, "Y": 0.0516
}

eiip_vectors = []

for seq in df_cf["Sequence"]:
    vec = [EIIP.get(a, 0.0) for a in seq]
    eiip_vectors.append(vec)

df_cf["EIIP"] = eiip_vectors
df_cf.head()

# Maksimum dizi uzunluğunu bul
max_len = max(len(v) for v in eiip_vectors)
print("Maksimum aminoasit uzunluğu:", max_len)

from tensorflow.keras.preprocessing.sequence import pad_sequences

X = pad_sequences(eiip_vectors, maxlen=max_len, padding='post', dtype='float32')

# EIIP vektörlerini DataFrame'e dönüştür (her sütun EIIP_1, EIIP_2, ...)
eiip_padded = X  # pad_sequences sonucu (num_samples x max_len)
eiip_df = pd.DataFrame(eiip_padded, columns=[f"EIIP_{i+1}" for i in range(eiip_padded.shape[1])])

# ProteinID ve Class sütunlarını başa ekle
eiip_df.insert(0, "ProteinID", df_cf["ProteinID"].values)
eiip_df.insert(1, "Class", df_cf["Class"].values)

eiip_output_path = "eiip_dataset.xlsx"
eiip_df.to_excel(eiip_output_path, index=False)
print(f"EIIP veri seti '{eiip_output_path}' dosyasına kaydedildi.")

y_str = df_cf["Class"].values
classes = sorted(df_cf["Class"].unique())
class_to_int = {c: i for i, c in enumerate(classes)}
int_to_class = {v: k for k, v in class_to_int.items()}

y = np.array([class_to_int[c] for c in y_str])

X.shape, y.shape, class_to_int

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train.shape, X_test.shape

# SVM modeli
svm_model = SVC(kernel="rbf", probability=True, random_state=42)
svm_model.fit(X_train, y_train)

# Test seti üzerinde tahmin
y_pred_svm = svm_model.predict(X_test)
y_proba_svm = svm_model.predict_proba(X_test)

print("SVM eğitimi bitti.")

from sklearn.metrics import classification_report

# Temel metrikler
acc_svm   = accuracy_score(y_test, y_pred_svm)
prec_svm  = precision_score(y_test, y_pred_svm, average="weighted")
rec_svm   = recall_score(y_test, y_pred_svm, average="weighted")
mcc_svm   = matthews_corrcoef(y_test, y_pred_svm)
kappa_svm = cohen_kappa_score(y_test, y_pred_svm)

# Multi-class AUC için: y'yi one-hot yap
y_test_bin = to_categorical(y_test, num_classes=len(class_to_int))
auc_svm = roc_auc_score(y_test_bin, y_proba_svm, multi_class="ovr")

print("=== SVM METRİKLERİ ===")
print("Accuracy :", acc_svm)
print("Precision:", prec_svm)
print("Recall   :", rec_svm)
print("MCC      :", mcc_svm)
print("Kappa    :", kappa_svm)
print("AUC      :", auc_svm)

print("\n=== SVM Classification Report ===")
print(classification_report(y_test, y_pred_svm, target_names=classes))

from sklearn.metrics import classification_report

# SVM raporunu .txt dosyasına yaz
svm_report_path = "svm_report.txt"
with open(svm_report_path, "w") as f:
    f.write("=== SVM METRİKLERİ ===\n")
    f.write(f"Accuracy : {acc_svm:.4f}\n")
    f.write(f"Precision: {prec_svm:.4f}\n")
    f.write(f"Recall   : {rec_svm:.4f}\n")
    f.write(f"MCC      : {mcc_svm:.4f}\n")
    f.write(f"Kappa    : {kappa_svm:.4f}\n")
    f.write(f"AUC      : {auc_svm:.4f}\n\n")

    f.write("=== SVM Classification Report ===\n")
    f.write(classification_report(y_test, y_pred_svm, target_names=classes))

print(f"SVM raporu '{svm_report_path}' dosyasına kaydedildi.")

metrics_svm = [acc_svm, prec_svm, rec_svm, mcc_svm, auc_svm, kappa_svm]
names_svm   = ["Acc","Prec","Rec","MCC","AUC","Kappa"]

plt.figure()
plt.bar(names_svm, metrics_svm)
plt.ylim(0, 1)
plt.title("SVM Performance Metrics")
plt.xlabel("Metric")
plt.ylabel("Score")
plt.tight_layout()

svm_metrics_plot_path = "svm_metrics_plot.png"
plt.savefig(svm_metrics_plot_path, dpi=300)
print(f"SVM metrik grafiği '{svm_metrics_plot_path}' dosyasına kaydedildi.")

plt.show()

cm_svm = confusion_matrix(y_test, y_pred_svm)

plt.figure()
plt.imshow(cm_svm, interpolation='nearest')
plt.title("SVM Confusion Matrix")
plt.colorbar()
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)

plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.tight_layout()

svm_cm_plot_path = "svm_confusion_matrix.png"
plt.savefig(svm_cm_plot_path, dpi=300)
print(f"SVM confusion matrix görseli '{svm_cm_plot_path}' dosyasına kaydedildi.")

plt.show()

print("Confusion Matrix (SVM):")
print(cm_svm)

# Girdiyi 3D hale getir (CNN için)
X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_cnn  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Çıktıyı one-hot yap
num_classes = len(class_to_int)
y_train_cnn = to_categorical(y_train, num_classes=num_classes)
y_test_cnn  = to_categorical(y_test, num_classes=num_classes)

X_train_cnn.shape, y_train_cnn.shape

model_cnn = Sequential([
    Conv1D(64, 3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    MaxPooling1D(2),
    Conv1D(128, 3, activation='relu'),
    MaxPooling1D(2),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model_cnn.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model_cnn.summary()

history = model_cnn.fit(
    X_train_cnn, y_train_cnn,
    epochs=30,
    batch_size=8,
    validation_data=(X_test_cnn, y_test_cnn),
    verbose=1
)

# Loss grafiği
plt.figure()
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title("1D-CNN Loss Grafiği")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.tight_layout()

cnn_loss_plot_path = "cnn_loss_plot.png"
plt.savefig(cnn_loss_plot_path, dpi=300)
print(f"1D-CNN loss grafiği '{cnn_loss_plot_path}' dosyasına kaydedildi.")

plt.show()

# Accuracy grafiği
plt.figure()
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Validation Acc')
plt.title("1D-CNN Accuracy Grafiği")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.tight_layout()

cnn_acc_plot_path = "cnn_accuracy_plot.png"
plt.savefig(cnn_acc_plot_path, dpi=300)
print(f"1D-CNN accuracy grafiği '{cnn_acc_plot_path}' dosyasına kaydedildi.")

plt.show()

plt.figure()
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Validation Acc')
plt.title("1D-CNN Accuracy Grafiği")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

y_proba_cnn = model_cnn.predict(X_test_cnn)
y_pred_cnn = np.argmax(y_proba_cnn, axis=1)

acc_cnn   = accuracy_score(y_test, y_pred_cnn)
prec_cnn  = precision_score(y_test, y_pred_cnn, average="weighted")
rec_cnn   = recall_score(y_test, y_pred_cnn, average="weighted")
mcc_cnn   = matthews_corrcoef(y_test, y_pred_cnn)
kappa_cnn = cohen_kappa_score(y_test, y_pred_cnn)

auc_cnn = roc_auc_score(y_test_cnn, y_proba_cnn, multi_class="ovr")

print("=== 1D-CNN METRİKLERİ ===")
print("Accuracy :", acc_cnn)
print("Precision:", prec_cnn)
print("Recall   :", rec_cnn)
print("MCC      :", mcc_cnn)
print("Kappa    :", kappa_cnn)
print("AUC      :", auc_cnn)

print("\n=== 1D-CNN Classification Report ===")
print(classification_report(y_test, y_pred_cnn, target_names=classes))

cnn_report_path = "cnn_report.txt"
with open(cnn_report_path, "w") as f:
    f.write("=== 1D-CNN METRİKLERİ ===\n")
    f.write(f"Accuracy : {acc_cnn:.4f}\n")
    f.write(f"Precision: {prec_cnn:.4f}\n")
    f.write(f"Recall   : {rec_cnn:.4f}\n")
    f.write(f"MCC      : {mcc_cnn:.4f}\n")
    f.write(f"Kappa    : {kappa_cnn:.4f}\n")
    f.write(f"AUC      : {auc_cnn:.4f}\n\n")

    f.write("=== 1D-CNN Classification Report ===\n")
    f.write(classification_report(y_test, y_pred_cnn, target_names=classes))

print(f"1D-CNN raporu '{cnn_report_path}' dosyasına kaydedildi.")

cm_cnn = confusion_matrix(y_test, y_pred_cnn)

plt.figure()
plt.imshow(cm_cnn, interpolation='nearest')
plt.title("1D-CNN Confusion Matrix")
plt.colorbar()
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)

plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.tight_layout()

cnn_cm_plot_path = "cnn_confusion_matrix.png"
plt.savefig(cnn_cm_plot_path, dpi=300)
print(f"1D-CNN confusion matrix görseli '{cnn_cm_plot_path}' dosyasına kaydedildi.")

plt.show()

print("Confusion Matrix (1D-CNN):")
print(cm_cnn)

fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_cnn[:, i], y_proba_cnn[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure()
for i in range(num_classes):
    plt.plot(fpr[i], tpr[i], label=f"Class {classes[i]} (AUC = {roc_auc[i]:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.title("1D-CNN ROC Eğrileri (One-vs-Rest)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.tight_layout()

cnn_roc_plot_path = "cnn_roc_curves.png"
plt.savefig(cnn_roc_plot_path, dpi=300)
print(f"1D-CNN ROC eğrileri '{cnn_roc_plot_path}' dosyasına kaydedildi.")

plt.show()